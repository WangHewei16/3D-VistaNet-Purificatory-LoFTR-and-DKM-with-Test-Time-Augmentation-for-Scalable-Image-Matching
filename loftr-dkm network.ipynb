{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2022-06-06T02:29:50.027632Z","iopub.status.busy":"2022-06-06T02:29:50.027311Z","iopub.status.idle":"2022-06-06T02:29:50.055085Z","shell.execute_reply":"2022-06-06T02:29:50.054283Z","shell.execute_reply.started":"2022-06-06T02:29:50.027549Z"},"trusted":true},"outputs":[],"source":["# Configuration environment: When the kaggle GPU is running, turn off the Internet\n","# Dataset link:\n","#https://www.kaggle.com/competitions/image-matching-challenge-2022\n","#https://www.kaggle.com/datasets/gufanmingmie/dkm-dependecies\n","#https://www.kaggle.com/datasets/ammarali32/kornia-loftr"]},{"cell_type":"markdown","metadata":{},"source":["# ***Install Libs***\n","### Install kornia&kornia_moons"]},{"cell_type":"code","execution_count":2,"metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2022-06-06T02:29:50.057815Z","iopub.status.busy":"2022-06-06T02:29:50.057333Z","iopub.status.idle":"2022-06-06T02:30:48.410229Z","shell.execute_reply":"2022-06-06T02:30:48.409271Z","shell.execute_reply.started":"2022-06-06T02:29:50.057779Z"},"trusted":true},"outputs":[],"source":["dry_run = False\n","!pip install ../input/kornia-loftr/kornia-0.6.4-py2.py3-none-any.whl\n","!pip install ../input/kornia-loftr/kornia_moons-0.1.9-py3-none-any.whl"]},{"cell_type":"markdown","metadata":{},"source":["# ***Import dependencies***"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-06-06T02:30:48.411994Z","iopub.status.busy":"2022-06-06T02:30:48.411720Z","iopub.status.idle":"2022-06-06T02:30:50.402549Z","shell.execute_reply":"2022-06-06T02:30:50.401857Z","shell.execute_reply.started":"2022-06-06T02:30:48.411958Z"},"trusted":true},"outputs":[],"source":["import os\n","import numpy as np\n","import cv2\n","import csv\n","from glob import glob\n","import torch\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","import kornia\n","from kornia_moons.feature import *\n","import kornia as K\n","import kornia.feature as KF\n","import gc\n","import sys, os, csv\n","sys.path.append('../input/dkm-dependecies/DKM/')"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-06-06T02:30:50.404710Z","iopub.status.busy":"2022-06-06T02:30:50.404444Z","iopub.status.idle":"2022-06-06T02:31:35.811286Z","shell.execute_reply":"2022-06-06T02:31:35.810434Z","shell.execute_reply.started":"2022-06-06T02:30:50.404677Z"},"trusted":true},"outputs":[],"source":["# Install dkm and copy the dkm pre-training weight to the local path\n","!mkdir -p pretrained/checkpoints\n","!cp ../input/dkm-dependecies/pretrained/dkm.pth pretrained/checkpoints/dkm_base_v11.pth\n","\n","!pip install -f ../input/dkm-dependecies/wheels --no-index einops\n","!cp -r ../input/dkm-dependecies/DKM/ /kaggle/working/DKM/\n","!cd /kaggle/working/DKM/; pip install -f ../input/dkm-dependecies/wheels -e . "]},{"cell_type":"markdown","metadata":{},"source":["# ***Model***"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-06-06T02:31:35.813184Z","iopub.status.busy":"2022-06-06T02:31:35.812883Z","iopub.status.idle":"2022-06-06T02:31:41.327332Z","shell.execute_reply":"2022-06-06T02:31:41.326368Z","shell.execute_reply.started":"2022-06-06T02:31:35.813148Z"},"trusted":true},"outputs":[],"source":["# loftr model loading and configuration parameters\n","device = torch.device('cuda') # use GPU\n","matcher = KF.LoFTR(pretrained=None) # loading LoFTR model structure\n","matcher.load_state_dict(torch.load(\"../input/kornia-loftr/loftr_outdoor.ckpt\")['state_dict']) # loading weight\n","matcher = matcher.to(device).eval() # transfer the model to GPU and start the evaluation mode\n","\n","IMG_MAX_SIZE = [1280, 840] # the maximum size of the long side of the picture, using two scales for TTA (test time augment) reasoning\n","DO_FLIP = True # turn on picture flipping\n","MAX_NUM_PAIRS = 8000 # maximum number of matching points\n","\n","# loading dkm model\n","torch.hub.set_dir('/kaggle/working/pretrained/')\n","from dkm import dkm_base\n","dkm_model = dkm_base(pretrained=True, version=\"v11\").to(device).eval()"]},{"cell_type":"markdown","metadata":{},"source":["## *Utils*"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-06-06T02:31:41.329134Z","iopub.status.busy":"2022-06-06T02:31:41.328879Z","iopub.status.idle":"2022-06-06T02:31:41.358186Z","shell.execute_reply":"2022-06-06T02:31:41.357463Z","shell.execute_reply.started":"2022-06-06T02:31:41.329101Z"},"trusted":true},"outputs":[],"source":["src = '/kaggle/input/image-matching-challenge-2022/'\n","\n","# Reading the testset picture data \n","test_samples = []\n","with open(f'{src}/test.csv') as f:\n","    reader = csv.reader(f, delimiter=',')\n","    for i, row in enumerate(reader):\n","        # Skip header.\n","        if i == 0:\n","            continue\n","        test_samples += [row]\n","\n","# Flatten the matrix to facilitate writing to CSV files\n","def FlattenMatrix(M, num_digits=8):\n","    '''Convenience function to write CSV files.'''\n","    \n","    return ' '.join([f'{v:.{num_digits}e}' for v in M.flatten()])\n","\n","\n","# loftr image loading function\n","def load_torch_image(fname, device, resize=True, img_max_size=1280):\n","    img = cv2.imread(fname)\n","    img_sz = img.shape\n","    if img_max_size <=0:\n","        resize = False\n","    # transform size\n","    if resize:\n","        # take the longest side as the determination scale (transform coefficient scale)\n","        scale = img_max_size / max(img.shape[0], img.shape[1]) \n","    else:\n","        scale = 1\n","    w = int(img.shape[1] * scale)\n","    h = int(img.shape[0] * scale)\n","    # image transformation, normalization, channel transformation\n","    img = cv2.resize(img, ((w//8)*8, (h//8)*8))\n","    img = K.image_to_tensor(img, False).float() /255.\n","    img = K.color.bgr_to_rgb(img)\n","    return img.to(device), img_sz\n","\n","# single pair picture matching function\n","def match_same_size(img_path0, img_path1, matcher, device=device, flip=False, img_max_size=1280):\n","    img0, source_img_shape = load_torch_image(img_path0, device, img_max_size=img_max_size)\n","    img1, target_img_shape = load_torch_image(img_path1, device, img_max_size=img_max_size)\n","    input_dict = {\"image0\": K.color.rgb_to_grayscale(img0).to(device), \n","                  \"image1\": K.color.rgb_to_grayscale(img1).to(device)}\n","    # whether to input the original drawing and reverse splicing\n","    if flip:\n","        input_dict[\"image0\"] = torch.cat((input_dict[\"image0\"], torch.flip(input_dict[\"image0\"], dims=[3])), dim=0)\n","        input_dict[\"image1\"] = torch.cat((input_dict[\"image1\"], torch.flip(input_dict[\"image1\"], dims=[3])), dim=0)\n","        \n","    # reasoning   \n","    with torch.no_grad():\n","        correspondences = matcher(input_dict)\n","    \n","    # flip coordinate\n","    # extract key point matching coordinate array\n","    # https://github.com/kornia/kornia/blob/master/kornia/feature/loftr/loftr.py\n","    keypoints0 = correspondences[\"keypoints0\"].cpu().numpy()\n","    keypoints1 = correspondences[\"keypoints1\"].cpu().numpy()\n","    confidence = correspondences[\"confidence\"].cpu().numpy()\n","    batch_indexes = correspondences[\"batch_indexes\"].cpu().numpy()\n","    if flip:\n","        select_fliped = (batch_indexes == 1)\n","        keypoints0[select_fliped, 0] = input_dict[\"image0\"].shape[-1] - keypoints0[select_fliped, 0]\n","        keypoints1[select_fliped, 0] = input_dict[\"image1\"].shape[-1] - keypoints1[select_fliped, 0]\n","    \n","    # scale\n","    # the point coordinates after the picture transformation need to be restored to the original picture coordinates\n","    new_source_img_shape = input_dict[\"image0\"].shape[-2:]\n","    new_target_img_shape = input_dict[\"image1\"].shape[-2:]\n","    sy, sx = source_img_shape[0] / new_source_img_shape[0], source_img_shape[1] / new_source_img_shape[1]\n","    keypoints0[:, 0] = keypoints0[:, 0] * sx\n","    keypoints0[:, 1] = keypoints0[:, 1] * sy\n","\n","    sy, sx = target_img_shape[0] / new_target_img_shape[0], target_img_shape[1] / new_target_img_shape[1]\n","    keypoints1[:, 0] = keypoints1[:, 0] * sx\n","    keypoints1[:, 1] = keypoints1[:, 1] * sy\n","    torch.cuda.empty_cache()\n","    return {\n","        \"keypoints0\": keypoints0,\n","        \"keypoints1\":  keypoints1,\n","        \"confidence\": confidence\n","    }\n","    \n","# wrap a single pair of picture functions into functions that can input pictures of multiple sizes   \n","def match(img_path0, img_path1, matcher, device=device):\n","    mkpts0 = []\n","    mkpts1 = []\n","    confidence = []\n","    for img_size in IMG_MAX_SIZE:\n","        preds =  match_same_size(img_path0, img_path1, matcher, device=device, flip=DO_FLIP, img_max_size=img_size)\n","        mkpts0.append(preds[\"keypoints0\"])\n","        mkpts1.append(preds[\"keypoints1\"])\n","        confidence.append(preds[\"confidence\"])\n","    # concatenate\n","    mkpts0 = np.concatenate(mkpts0, axis=0)\n","    mkpts1 = np.concatenate(mkpts1, axis=0)\n","    confidence = np.concatenate(confidence, axis=0)\n","    \n","    # filter pairs\n","    # filter matching point pairs with low scores, and the maximum value is Max_ NUM_ PAIRS\n","    ind = np.argsort(-confidence)\n","    ind = ind[:MAX_NUM_PAIRS]\n","    mkpts0 = mkpts0[ind]\n","    mkpts1 = mkpts1[ind]\n","    confidence = confidence[ind]\n","    return mkpts0, mkpts1"]},{"cell_type":"markdown","metadata":{},"source":["# ***Inference***"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-06-06T02:31:41.360113Z","iopub.status.busy":"2022-06-06T02:31:41.359607Z","iopub.status.idle":"2022-06-06T02:32:42.590286Z","shell.execute_reply":"2022-06-06T02:32:42.589361Z","shell.execute_reply.started":"2022-06-06T02:31:41.360058Z"},"trusted":true},"outputs":[],"source":["F_dict = {}\n","import time\n","for i, row in enumerate(test_samples):\n","    # acquisition of data information from pictures of the current batch\n","    sample_id, batch_id, image_1_id, image_2_id = row\n","    \n","    #loftr model extracts keypoint-pairs array\n","    st = time.time()\n","    img_path0 = f'{src}/test_images/{batch_id}/{image_1_id}.png'\n","    img_path1 = f'{src}/test_images/{batch_id}/{image_2_id}.png'\n","    mkpts0, mkpts1 = match(img_path0, img_path1, matcher)\n","    torch.cuda.empty_cache()\n","    \n","    ##dkm\n","    img1 = cv2.imread(img_path0) \n","    img2 = cv2.imread(img_path1)\n","        \n","    img1PIL = Image.fromarray(cv2.cvtColor(img1, cv2.COLOR_BGR2RGB))\n","    img2PIL = Image.fromarray(cv2.cvtColor(img2, cv2.COLOR_BGR2RGB))\n","    \n","    # dkm model extracts keypoint-pairs\n","    dense_matches, dense_certainty = dkm_model.match(img1PIL, img2PIL)\n","    dense_certainty = dense_certainty.sqrt()\n","    sparse_matches, sparse_certainty = dkm_model.sample(dense_matches, dense_certainty, 2000)\n","    # filter point pairs with low matching scores\n","    sparse_matches = sparse_matches[sparse_certainty>0.5]\n","    dkm_p0 = sparse_matches[:, :2]\n","    dkm_p1 = sparse_matches[:, 2:]\n","    \n","    # coordinate restore\n","    h, w, c = img1.shape\n","    dkm_p0[:, 0] = ((dkm_p0[:, 0] + 1)/2) * w\n","    dkm_p0[:, 1] = ((dkm_p0[:, 1] + 1)/2) * h\n","\n","    h, w, c = img2.shape\n","    dkm_p1[:, 0] = ((dkm_p1[:, 0] + 1)/2) * w\n","    dkm_p1[:, 1] = ((dkm_p1[:, 1] + 1)/2) * h\n","    \n","    # splice the prediction results of the two models\n","    mkpts0 = np.concatenate([mkpts0, dkm_p0], axis=0)\n","    mkpts1 = np.concatenate([mkpts1, dkm_p1], axis=0)\n","    # at least 8 points are required to obtain the basic matrix (using the 8-point algorithm)\n","    if len(mkpts0) > 7:\n","        # use cv2.findFundamentalMat to calculate the basic matrix, and use cv2.USAC_MAGSAC estimator, the matching error is 0.1845-0.99999,\n","        # and the maximum number of iterations is 10000\n","        F, inliers = cv2.findFundamentalMat(mkpts0, mkpts1, cv2.USAC_MAGSAC, 0.1845, 0.999999, 10000)\n","        inliers = inliers > 0\n","        assert F.shape == (3, 3), 'Malformed F?'\n","        F_dict[sample_id] = F\n","    else:\n","        F_dict[sample_id] = np.zeros((3, 3))\n","        continue\n","    gc.collect()\n","    nd = time.time()    \n","    # drawing correlation function\n","    if (i < 3):\n","        print(\"Running time: \", nd - st, \" s\")\n","        image_1 =load_torch_image(img_path0, device=device, resize=False)[0]\n","        image_2 = load_torch_image(img_path1, device=device,  resize=False)[0]\n","        draw_LAF_matches(\n","        KF.laf_from_center_scale_ori(torch.from_numpy(mkpts0).view(1,-1, 2),\n","                                    torch.ones(mkpts0.shape[0]).view(1,-1, 1, 1),\n","                                    torch.ones(mkpts0.shape[0]).view(1,-1, 1)),\n","\n","        KF.laf_from_center_scale_ori(torch.from_numpy(mkpts1).view(1,-1, 2),\n","                                    torch.ones(mkpts1.shape[0]).view(1,-1, 1, 1),\n","                                    torch.ones(mkpts1.shape[0]).view(1,-1, 1)),\n","        torch.arange(mkpts0.shape[0]).view(-1,1).repeat(1,2),\n","        K.tensor_to_image(image_1),\n","        K.tensor_to_image(image_2),\n","        inliers,\n","        draw_dict={'inlier_color': (0.2, 1, 0.2),\n","                   'tentative_color': None, \n","                   'feature_color': (0.2, 0.5, 1), 'vertical': False})\n","    \n","# save result file\n","with open('submission.csv', 'w') as f:\n","    f.write('sample_id,fundamental_matrix\\n')\n","    for sample_id, F in F_dict.items():\n","        f.write(f'{sample_id},{FlattenMatrix(F)}\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
